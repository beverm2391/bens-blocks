import Image from 'next/image'
import { Callout } from 'nextra-theme-docs'

# How To "Train" GPT-3 on Your Own Data

Alright - we're hopping straight into it. As a developer, I always get aggravated when I see clickbait tutorials that don't actually teach you how stuff works. So, I'm going to try and make this technical but also accessible. 

In this tutorial, it's actually not correct to say we're going to "train" GPT-3. In reality, we're going to build a corpus of embedded text that GPT-3 will automatically select from to generate its answer. This is by far the best way to get GPT-3 to generate answers that are relevant to your specific domain - taken directly from the [OpenAI Cookbook]().

### What's the difference?

Well, traditionally the word "train" is used to describe fine-tuning. Fine-tuning is when you take a neural network and update its weights by feeding it a bunch of specific data. This can be done through techniques like [gradient descent](), [backpropagation](), and so on. If you really want to get down and dirty with [deep learning](), read this book, *[Grokking Deep Learning]()*.

Why not fine-tune GPT-3 you ask? Well, if you've read [its inaugural paper](), you'll know that GPT-3 was designed to be **task agnostic**, unlike previous LLMs. This means that it was optimized to perform well on a variety of benchmarks without requiring any fine-tuning. The problem is, if you fine-tune a model to perform well on a specific task, it will perform poorly on other tasks. This is called [catastrophic forgetting]().

So, a few important things to note:
- GPT-3 is task-agnostic by design, and fine-tuning requires tons of data
- Instead, we're going to use vector embeddings to encode our data and feed it to GPT-3
- This is a much more efficient way (at small scale) than fine-tuning.

Here's a diagram:

<Image src="/images/train-gpt-3-pipeline.png" width={700} height={200} className='image'/>

### What's a vector embedding?

A vector embedding is a way of representing some text as a vector of numbers. This is a very common technique in natural language processing (NLP). The idea is that once you represent some text as a vector of numbers, and then you can perform mathematical operations on these vectors to measure how similar they are (by sentiment).

## Step 1: Create a Corpus

